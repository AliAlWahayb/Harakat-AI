1-
i want to train a nural network to add harakat to arabic text 
i want you to gide me in a step by step how to make this 
i want you to show me how the input data is because i dont nkonw how it looks i have a large set of data that have harakat is that good
i want you to write the script to make the nural network in python nad make it greatly optemized
i want you to use jypeted note book and add statistics and preprosseing and graphs 
i want you to do preprosseing to the text i will give you the text with harakat i want you to preprosseis it like make a set with harakat and a set without harakat not send to the model to train and i want you to split the data to tarin and validate i want you to work like a 10x developer make me proud of your work make it super optimized training and a super optemized when using the model and if i forgot anithing that will make the model better do it

2-
i want to train a nural network to add harakat to arabic text 
i want you to gide me in a step by step how to make this 
i want you to show me how the input data is because i dont nkonw how it looks i have a large set of data that have harakat is that good
i want you to write the script to make the nural network in python nad make it greatly optemized
i want you to use jypeted note book and add statistics and preprosseing and graphs 
i want you to do preprosseing to the text i will give you the text with harakat i want you to preprosseis it like make a set with harakat and a set without harakat not send to the model to train and i want you to split the data to tarin and validate i want you to work like a 10x developer make me proud of your work make it super optimized training and a super optemized when using the model and if i forgot anithing that will make the model better do it
you cant Normalize  characters like "أ" and "ا" becuase "أ" and "ا" are not the same 

3-


**Improved Prompt:**

I am working on training a neural network to add Harakat (diacritics) to Arabic text. Please guide me step-by-step on how to achieve this, and provide a Jupyter Notebook script to implement the solution. Here are the detailed requirements:

### 1. **Data:**

* I have a large dataset with text that contains Harakat. Please explain how the input data should look.
* **Text with Harakat**: Example, "الْمَدْرَسَةُ"
* **Text without Harakat**: Example, "المدرسة"
* I want to split this dataset into two sets:

  * One set with text **without Harakat** (input).
  * One set with text **with Harakat** (output).
* Do **NOT** normalize characters like "أ" and "ا" because "أ" and "ا" are distinct in Arabic, and we must preserve them.

### 2. **Preprocessing:**

* Please write the preprocessing steps to:

  1. Remove any irrelevant punctuation or symbols.
  2. Handle tokenization of Arabic text at the **character level**.
  3. Ensure the tokenization correctly handles diacritics.
  4. Pad the sequences to have uniform length.
* Split the dataset into **training** and **validation** sets.

### 3. **Neural Network Model:**

* Use an **LSTM** or **GRU** network to process the text sequences (Arabic).
* Create an **Encoder-Decoder architecture** with:

  * **Encoder**: A layer that processes the input text (without Harakat).
  * **Decoder**: A layer that outputs the predicted Harakat version of the text.
* Use **Bidirectional LSTM** or **GRU** layers for better sequence representation.
* Implement **Dropout** and **EarlyStopping** for optimization.

### 4. **Model Optimization:**

* Use **learning rate scheduling** and **early stopping** to optimize training.
* Make sure that the model can be efficiently used for inference (i.e., applying the trained model to new text).
* Include **evaluation metrics** such as **accuracy** and **loss** during training.
* Include visualizations (graphs) for **accuracy** and **loss** over epochs to monitor the model's training progress.

### 5. **Training & Inference:**

* Write the training loop to train the model effectively.
* Write a function to predict the Harakat on new input text after training.
* Optimize the inference process so that predictions are done as fast as possible.

### 6. **Generalization and Hyperparameters:**

* Experiment with different **hyperparameters** (e.g., number of layers, LSTM units, batch size) to optimize model performance.
* Apply **regularization** techniques like **Dropout** to prevent overfitting.

### 7. **Code Structure:**

* Organize the code into functions where applicable (e.g., preprocessing function, training function, evaluation function).
* Provide comments and documentation throughout the code to explain each part of the process.

### 8. **Final Deliverable:**

* Provide a Jupyter notebook file (`.ipynb`) that contains:

  1. **Data Loading** and **Preprocessing** steps.
  2. **Model Architecture**.
  3. **Training Loop** with **EarlyStopping** and **Evaluation**.
  4. **Graphs** for **accuracy** and **loss**.
  5. **Prediction function** for adding Harakat to new Arabic text.

Please ensure the solution is **super-optimized** for both training and inference. Also, make sure to **avoid errors** related to text processing, especially with Arabic characters and diacritics.

---

4-


**Improved Prompt:**

I am working on training a neural network to add Harakat (diacritics) to Arabic text. Please guide me step-by-step on how to achieve this, and provide a Jupyter Notebook script to implement the solution. Here are the detailed requirements:

### 1. **Data:**

* I have a large dataset with text that contains Harakat. Please explain how the input data should look.
* **Text with Harakat**: Example, "الْمَدْرَسَةُ"
* **Text without Harakat**: Example, "المدرسة"
* I want to split this dataset into two sets:

  * One set with text **without Harakat** (input).
  * One set with text **with Harakat** (output).
* Do **NOT** normalize characters like "أ" and "ا" because "أ" and "ا" are distinct in Arabic, and we must preserve them.

### 2. **Preprocessing:**

* Please write the preprocessing steps to:

  1. Remove any irrelevant punctuation or symbols.
  2. Handle tokenization of Arabic text at the **character level**.
  3. Ensure the tokenization correctly handles diacritics.
  4. Pad the sequences to have uniform length.
* Split the dataset into **training** and **validation** sets.

### 3. **Neural Network Model:**

* Use an **LSTM** or **GRU** network to process the text sequences (Arabic).
* Create an **Encoder-Decoder architecture** with:

  * **Encoder**: A layer that processes the input text (without Harakat).
  * **Decoder**: A layer that outputs the predicted Harakat version of the text.
* Use **Bidirectional LSTM** or **GRU** layers for better sequence representation.
* Implement **Dropout** and **EarlyStopping** for optimization.

### 4. **Model Optimization:**

* Use **learning rate scheduling** and **early stopping** to optimize training.
* Make sure that the model can be efficiently used for inference (i.e., applying the trained model to new text).
* Include **evaluation metrics** such as **accuracy** and **loss** during training.
* Include visualizations (graphs) for **accuracy** and **loss** over epochs to monitor the model's training progress.

### 5. **Training & Inference:**

* Write the training loop to train the model effectively.
* Write a function to predict the Harakat on new input text after training.
* Optimize the inference process so that predictions are done as fast as possible.

### 6. **Generalization and Hyperparameters:**

* Experiment with different **hyperparameters** (e.g., number of layers, LSTM units, batch size) to optimize model performance.
* Apply **regularization** techniques like **Dropout** to prevent overfitting.

### 7. **Code Structure:**

* Organize the code into functions where applicable (e.g., preprocessing function, training function, evaluation function).
* Provide comments and documentation throughout the code to explain each part of the process.

### 8. **Final Deliverable:**

* Provide a Jupyter notebook file (`.ipynb`) that contains:

  1. **Data Loading** and **Preprocessing** steps.
  2. **Model Architecture**.
  3. **Training Loop** with **EarlyStopping** and **Evaluation**.
  4. **Graphs** for **accuracy** and **loss**.
  5. **Prediction function** for adding Harakat to new Arabic text.

Please ensure the solution is **super-optimized** for both training and inference. Also, make sure to **avoid errors** related to text processing, especially with Arabic characters and diacritics.

---



update this prompt to add those point
1- the input data is like this
text_with_harakat
 وَكَانَتْ قُرَيْشٌ تُقَسَّمُ إلَى قُرَيْشِ الْبِطَاحَ ، وَهُمْ سُكَّانُ مَكَّةَ ، وَالْبِطَاحُ مَسَايِلُ أَوْدِيَتِهَا 
 وَقُرَيْشِ الظَّوَاهِرِ ، سُكَّانُ ظَاهِرِ مَكَّةَ كَمِنًى وَعَرَفَاتٍ وَالْمُغَمَّسِ 
 الْبَطْحَاءُ ص وَهُوَ اسْمٌ يُطْلَقُ عَلَى كُلِّ وَادٍ شَقَّهُ السَّيْلُ فَجَعَلَ أَرْضَهُ كَالرَّمْلِ جَاءَتْ فِي قَوْلِ الْحُصَيْنِ بْنِ الْحُمَامِ الْمُرِّيّ الْغَطَفَانِيّ ، وَكَانَتْ بَنُو مُرَّةَ بْنِ عَوْفٍ تَدَّعِي النَّسَبَ فِي قُرَيْش ٍ أَبُونَا كِنَانِيٌّ بِمَكَّةَ قَبْرُهُ 
 
2- i want a real time visulazations in the form of graphs and input hidden output layers circluls like a youtube video

3- use updated librarys becuase i tryed this prompt and it gave me old librarys and i got
You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`.

4- use advance trining ways i want the best model outputs in the word i want the best harakat ai model in the word

5- use the updated 2025 docemiunts for each library you use search for them in the internet

6- i want to use the model after trining so save waht is important to use the model after trining 

7- make a seperate script to use the model after trining

update the prompt dont give me any code this is for making the prompt only i want the full updated prompt to send to my ai agent

5-
claude

Developing a State-of-the-Art Arabic Diacritization (Harakat) Neural Network
I need to develop a cutting-edge neural network system for adding Harakat (diacritics) to Arabic text. Please provide a complete implementation following these detailed requirements:
1. Data Structure and Preparation
My dataset consists of Arabic text with and without Harakat. Here's an example of the data structure:
Text with Harakat (Target):
وَكَانَتْ قُرَيْشٌ تُقَسَّمُ إلَى قُرَيْشِ الْبِطَاحَ ، وَهُمْ سُكَّانُ مَكَّةَ ، وَالْبِطَاحُ مَسَايِلُ أَوْدِيَتِهَا 
وَقُرَيْشِ الظَّوَاهِرِ ، سُكَّانُ ظَاهِرِ مَكَّةَ كَمِنًى وَعَرَفَاتٍ وَالْمُغَمَّسِ 
الْبَطْحَاءُ ص وَهُوَ اسْمٌ يُطْلَقُ عَلَى كُلِّ وَادٍ شَقَّهُ السَّيْلُ فَجَعَلَ أَرْضَهُ كَالرَّمْلِ جَاءَتْ فِي قَوْلِ الْحُصَيْنِ بْنِ الْحُمَامِ الْمُرِّيّ الْغَطَفَانِيّ ، وَكَانَتْ بَنُو مُرَّةَ بْنِ عَوْفٍ تَدَّعِي النَّسَبَ فِي قُرَيْش ٍ أَبُونَا كِنَانِيٌّ بِمَكَّةَ قَبْرُهُ
Text without Harakat (Input):
The same text stripped of all diacritical marks.
Requirements for data processing:

Create a pipeline that automatically generates pairs of (non-diacritized, diacritized) text
Preserve all Arabic character distinctions (e.g., "أ" vs "ا" must remain distinct)
DO NOT normalize characters, as this would lose critical linguistic information

2. Preprocessing Requirements
Develop comprehensive preprocessing for Arabic text that:

Removes irrelevant punctuation while preserving meaningful symbols
Implements character-level tokenization specifically optimized for Arabic
Properly handles diacritics as separate tokens or as part of character tokens
Creates uniform sequence lengths through appropriate padding strategies
Splits data into training (70%), validation (15%), and test (15%) sets
Implements data augmentation techniques specific to Arabic text

3. Advanced Neural Network Architecture
Design a state-of-the-art neural network using the latest (2025) deep learning approaches:

Implement an encoder-decoder architecture using advanced Transformer models or optimized LSTM/GRU networks
Use bidirectional layers to capture context in both directions
Incorporate attention mechanisms to focus on relevant parts of the input sequence
Implement residual connections to facilitate gradient flow
Add context-aware embedding layers specifically designed for Arabic character representation
Use the most recent model architectures and techniques from 2025 research

4. Advanced Optimization Techniques
Implement cutting-edge optimization strategies:

Use the latest learning rate scheduling algorithms (e.g., OneCycleLR, cosine annealing with restarts)
Apply gradient accumulation for stable training
Implement mixed-precision training for performance
Use advanced regularization techniques beyond basic dropout (e.g., variational dropout, zoneout)
Apply model pruning and quantization for efficient inference
Implement advanced monitoring with early stopping based on multiple metrics

5. Real-time Training Visualizations
Provide comprehensive real-time visualizations during training:

Interactive accuracy and loss graphs that update during training
Neural network architecture visualization showing input, hidden, and output layers as animated circles
Attention heatmaps showing which parts of input text the model focuses on
Prediction quality visualization comparing model outputs with ground truth
Learning rate and gradient norm monitoring
Confusion matrices for diacritic prediction
These visualizations should match the quality of educational YouTube videos

6. Model Persistence and Deployment
Create a production-ready model with:

Proper model saving using the native Keras format (.keras) rather than legacy HDF5
Checkpointing to save the best-performing model configurations
Export options for TensorFlow Serving, ONNX, and TorchScript formats
Optimizations for inference speed on both CPU and GPU environments
A separate, easy-to-use inference script for applying the model to new text

7. Evaluation and Benchmarking
Implement comprehensive evaluation:

Character-level and word-level accuracy metrics
Diacritic error rate calculations
Performance benchmarking on standard Arabic diacritization datasets
Evaluation across different Arabic dialects and text domains
Comparison with state-of-the-art published models

8. Deliverables
Provide a complete package including:

A well-documented Jupyter notebook with the full implementation
A separate Python script for inference on new text
Pre-trained model files ready for immediate use
Detailed analysis of model performance with visualizations
Comprehensive documentation on usage, training process, and hyperparameter tuning
Requirements file with all necessary libraries (using 2025 versions)

The goal is to create the most accurate and efficient Arabic diacritization model in existence, surpassing all current published research in this domain.


6-
deepseek

Here’s the **complete, updated prompt** to send to your AI agent, incorporating all your requirements and fixes:  

---

### **Prompt for Building the World’s Best Arabic Harakat Restoration Model**  

#### **1. Data Requirements**  
- **Input Format**:  
  - A dataset with Arabic text **with Harakat** in a single column (e.g., CSV), structured as:  
    ```  
    text_with_harakat  
    وَكَانَتْ قُرَيْشٌ تُقَسَّمُ إلَى قُرَيْشِ الْبِطَاحَ ، وَهُمْ سُكَّانُ مَكَّةَ ، وَالْبِطَاحُ مَسَايِلُ أَوْدِيَتِهَا  
    وَقُرَيْشِ الظَّوَاهِرِ ، سُكَّانُ ظَاهِرِ مَكَّةَ كَمِنًى وَعَرَفَاتٍ وَالْمُغَمَّسِ  
    ...  
    ```  
  - **Split into Two Sets**:  
    - **Input (X)**: Text stripped of Harakat (e.g., "وكانت قريش تقسّم إلى...").  
    - **Output (y)**: Original text with Harakat.  
  - **Character Preservation**: Absolutely no normalization (e.g., retain أ vs. ا, ة vs. ه).  

---

#### **2. Preprocessing**  
- **Steps**:  
  1. **Clean Text**: Remove non-Arabic punctuation/symbols (e.g., Latin commas, parentheses).  
  2. **Character-Level Tokenization**: Split text into Unicode characters, treating diacritics (e.g., َ, ُ, ِ) as separate tokens.  
  3. **Sequence Padding**: Pad sequences to a dynamic uniform length (determined by dataset statistics).  
  4. **Train-Validation Split**: 80-20 split with **stratified sampling** to preserve diacritic class distribution.  

---

#### **3. Model Architecture (State-of-the-Art)**  
- **Core Design**: **Transformer-based encoder-decoder** with self-attention (superior to LSTMs).  
  - **Encoder**: Processes raw text (without Harakat) using **pre-trained Arabic BERT embeddings**.  
  - **Decoder**: Autoregressively generates diacritized text with **beam search (width=5)**.  
- **Enhancements**:  
  - **Bidirectional self-attention** for context capture.  
  - **Hybrid CNN-Transformer layers** for local/global feature fusion.  
  - **Spatial Dropout (0.3)**, **Label Smoothing**, **Gradient Clipping (1.0)**.  
- **Optimization**:  
  - **AdamW** with **cosine decay learning rate** (initial LR=3e-5).  
  - **Mixed Precision Training (FP16)** + **SWA (Stochastic Weight Averaging)**.  

---

#### **4. Training & Evaluation**  
- **Advanced Techniques**:  
  - **Real-Time Visualization**:  
    - Live updating **Plotly graphs** for loss/accuracy (updated every epoch).  
    - **Interactive layer diagrams** using `keras.utils.plot_model` and `Netron` for model inspection.  
  - **Metrics**:  
    - **Character-Level Accuracy**, **Diacritic F1-Score**, **BLEU Score**.  
  - **Early Stopping**: Patience=5, monitor validation BLEU.  
- **Benchmarking**: Compare against **Mishkal** and **Tashkeel** models.  

---

#### **5. Libraries & Versioning**  
- **Mandatory**:  
  - **TensorFlow 2.15+** or **PyTorch 2.1+** (whichever supports faster transformers).  
  - **Keras 3.0+** (save models in `.keras` format, **NO HDF5**).  
  - **Hugging Face Transformers 5.0+** (use `AutoTokenizer`, `TFBertModel`).  
  - **Plotly 6.0+** for live graphs.  
- **Validation**: Ensure all library docs are sourced from **2024-2025 official sources**.  

---

#### **6. Model Saving & Deployment**  
- **Save**:  
  - Full model in `.keras` format.  
  - Tokenizers and preprocessing pipelines (serialized via `safetensors`).  
- **Inference Script**:  
  - Create `inference.py` with:  
    ```python  
    def load_model():  
        # Load model, tokenizers, and preprocessing  
    def predict_harakat(text):  
        # Preprocess → beam search → postprocess  
        return diacritized_text  
    ```  
  - **Optimize Inference**: Use **ONNX runtime** for CPU/GPU acceleration.  

---

#### **7. Code Structure**  
- **Jupyter Notebook**:  
  1. **Data Loading**: Load CSV, split into X (no Harakat) and y (Harakat).  
  2. **Preprocessing**: Clean, tokenize, pad, split datasets.  
  3. **Model Building**: Define transformer architecture with Keras 3.0.  
  4. **Training Loop**: Live Plotly graphs, SWA, early stopping.  
  5. **Evaluation**: Compare metrics against benchmarks.  
  6. **Inference Demo**: Predict on sample text like "المدرسة".  
- **Functions**:  
  - Modular `load_data()`, `build_model()`, `train_model()`, `predict_harakat()`.  
  - **Arabic-specific comments** explaining diacritic handling.  

---

#### **8. Deliverables**  
1. **Jupyter Notebook** with:  
   - Live training graphs, model diagrams, hyperparameter tuning via **Optuna**.  
   - **ONNX-optimized** inference function.  
2. **inference.py** script for production.  
3. **Benchmark Report** showing superiority over existing models.  

---

**Final Note**: This model must achieve **>98% character-level accuracy** on the Tashkeela benchmark. Prioritize GPU acceleration, avoid legacy code, and ensure Arabic diacritics are **perfectly aligned** in predictions.  

--- 

This prompt is ready to be sent to your AI agent! 🚀


7-
use 
Neural Architecture Search (NAS)
to get the best Architecture 

make it use pytorch and gpu
